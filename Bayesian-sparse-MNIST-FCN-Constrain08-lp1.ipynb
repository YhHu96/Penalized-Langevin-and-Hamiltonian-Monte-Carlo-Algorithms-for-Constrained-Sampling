{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef704552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levy_stable\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48181b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple fully connected neural network\n",
    "class simpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=28*28 , width=400, depth=4, num_classes=10):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.width, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *layers,\n",
    "            nn.Linear(self.width, self.num_classes, bias=False),\n",
    "        )\n",
    "\n",
    "    def get_layers(self):\n",
    "        layers = []\n",
    "        for i in range(self.depth - 2):\n",
    "            layers.append(nn.Linear(self.width, self.width, bias=False))\n",
    "            layers.append(nn.ReLU())\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.input_dim)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2af361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "batch_size=128\n",
    "data_tf = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=(0.1307,), std=(0.3081,))])\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=data_tf, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=data_tf)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09120749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    SGLD optimiser based on pytorch's SGD.\n",
    "    Note that the weight decay is specified in terms of the gaussian prior sigma.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, norm_sigma=0, addnoise=True):\n",
    "\n",
    "        weight_decay = 1 / (norm_sigma ** 2)\n",
    "\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, addnoise=addnoise)\n",
    "\n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                if group['addnoise']:\n",
    "\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5 * d_p + langevin_noise)\n",
    "                else:\n",
    "                    p.data.add_(-group['lr'], 0.5 * d_p)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07492e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class PSGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    Penalized SGLD optimiser based on pytorch's SGD.\n",
    "    Note that the weight decay is specified in terms of the gaussian prior sigma.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, delta, constrain_list, lp = 1):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        defaults = dict(lr=lr, delta=delta, constrain_list = constrain_list, lp = lp)\n",
    "        super(PSGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PSGLD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "            \n",
    "#     def phi0(self, t, gamma):\n",
    "#         return torch.exp(-t * gamma)\n",
    "\n",
    "#     def phi1(self, t, gamma):\n",
    "#         return - 1/gamma * torch.exp(-t * gamma) + 1 / gamma\n",
    "\n",
    "#     def phi2(self, t, gamma):\n",
    "#         tmp = 1/gamma ** 2\n",
    "#         tmp = tmp * (np.torch(-gamma*t)-1)\n",
    "#         tmp = tmp + t/gamma\n",
    "#         return tmp\n",
    "\n",
    "#     def covariance(self, t, gamma):\n",
    "#         e2gt = torch.exp(-2*gamma*t)\n",
    "#         egt = torch.exp(-gamma*t)\n",
    "#         a11 = - (1/(2*gamma)) * e2gt + 1/(2*gamma)\n",
    "#         a12 = e2gt / (2*gamma*gamma) - egt/(gamma**2) - 1/(2*(gamma**2)) + 1/(gamma ** 2)\n",
    "#         a22 = -e2gt/(2*(gamma ** 3)) + 2*egt / (gamma**3) + 1/(2 * (gamma ** 3)) - 2/(gamma ** 3) + t/(gamma ** 2)\n",
    "#         tmp = torch.tensor([[a11, a12], [a12,a22]])\n",
    "#         return tmp\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            delta = group['delta']\n",
    "            constrain_list = group['constrain_list']\n",
    "            lp = group['lp']\n",
    "            \n",
    "            idx = 0\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                norm_ord = torch.norm(p.data, p = lp)\n",
    "                constrain = constrain_list[idx]\n",
    "                if norm_ord > constrain:\n",
    "                    g1 = torch.pow(torch.abs(p.data)/norm_ord, lp-1)\n",
    "                    tmp_con = (norm_ord-constrain) * (g1 * torch.sign(p.data))\n",
    "                    d_p.add_(delta, tmp_con)\n",
    "                \n",
    "\n",
    "                p.data.add_(-group['lr'], d_p)\n",
    "                noise = torch.normal(0,1,size = p.data.shape).cuda()\n",
    "                p.data.add_(torch.sqrt(torch.tensor(2*group['lr'])), noise)\n",
    "                idx += 1\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c14147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type simpleNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7912166666666667\n",
      "0.8519166666666667\n",
      "0.8799833333333333\n",
      "0.89265\n",
      "1\n",
      "0.7782166666666667\n",
      "0.8503\n",
      "0.8776666666666667\n",
      "0.8885\n",
      "2\n",
      "0.7739666666666667\n",
      "0.8504666666666667\n",
      "0.8769166666666667\n",
      "0.8854333333333333\n",
      "3\n",
      "0.7845833333333333\n",
      "0.84895\n",
      "0.8788666666666667\n",
      "0.8858166666666667\n",
      "4\n",
      "0.7784333333333333\n",
      "0.85865\n",
      "0.88205\n",
      "0.8950666666666667\n"
     ]
    }
   ],
   "source": [
    "lr_list = [3*(0.1)**7]\n",
    "delta = 500\n",
    "\n",
    "# 1-norm of the result from SGLD, used as the constraints in the constrained network\n",
    "norm_list = torch.tensor([94749.414 , 48327.09  , 48311.508 ,  1030.1956])\n",
    "# we can change the value of s here\n",
    "constrain_list = norm_list * 0.8\n",
    "\n",
    "# select 1-norm\n",
    "lp = 1\n",
    "epoch = 400\n",
    "# calculate the averaged result from 5 runs\n",
    "N = 5\n",
    "\n",
    "# path to save models\n",
    "PATH = './MNIST-3FCN-Pen/constrain08/SGLD-lp1/'\n",
    "try:\n",
    "    os.mkdir(PATH)\n",
    "except OSError as exc:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_acc_all = []\n",
    "\n",
    "for n in range(N):\n",
    "    print(n)\n",
    "    for i in range(len(lr_list)):\n",
    "        learning_rate = lr_list[i]\n",
    "        trainErrorList=[]\n",
    "        trainAccList=[]\n",
    "\n",
    "    \n",
    "        model = simpleNet()\n",
    "        if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "        criterion = nn.functional.nll_loss\n",
    "#         optimizer = SGLD(model.parameters(), lr = learning_rate, norm_sigma = 1, addnoise = True)\n",
    "        optimizer = PSGLD(model.parameters(), lr=learning_rate, delta = delta, constrain_list = constrain_list, lp = lp)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "        \n",
    "        for l in range(epoch):\n",
    "            train_acc=0\n",
    "            for data in train_loader:\n",
    "                img, label = data\n",
    "                img=img.view(img.size(0),-1)\n",
    "                if torch.cuda.is_available():\n",
    "                    img = img.cuda()\n",
    "                    label = label.cuda()\n",
    "                else:\n",
    "                    img = Variable(img)\n",
    "                    label = Variable(label)\n",
    "                out = model(img)\n",
    "                loss = F.cross_entropy(out, label, reduction='sum')\n",
    "                print_loss = loss.data.item()\n",
    "                _, pred = torch.max(out.data, 1)\n",
    "                train_acc += pred.eq(label.view_as(pred)).sum().item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            trainErrorList.append(loss.data.item())\n",
    "            trainAccList.append(train_acc/60000)\n",
    "            scheduler.step()\n",
    "            if l % 100 == 0 and l !=0:\n",
    "                print(train_acc/60000)\n",
    "            if (train_acc/60000) >=0.7 and train_acc/60000 == max(trainAccList):\n",
    "                tmp_path = PATH + '/model' + '{}'.format(n) +'.pth'\n",
    "                torch.save(model, tmp_path)\n",
    "        train_acc_all.append(trainAccList)\n",
    "        print(train_acc/60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b0204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7999, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "norms = []\n",
    "norm_cal = []\n",
    "for p in model.parameters():\n",
    "#         if p.grad is None:\n",
    "#             continue\n",
    "#         else:\n",
    "    norm_ord = torch.norm(p.data, p = lp).detach()\n",
    "    norm_cal.append(norm_ord)\n",
    "norms.append(norm_cal)\n",
    "norm = np.linalg.norm(norms, ord = lp, axis = 1)\n",
    "print(np.max(norm)/ torch.norm(torch.tensor([94749.414 , 48327.09  , 48311.508 ,  1030.1956]),p = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64091f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
